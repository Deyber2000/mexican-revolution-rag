# Test Suite Documentation

This directory contains comprehensive unit tests for the Mexican Revolution RAG Conversational Agent.

## ðŸ“ Test Structure

```
tests/
â”œâ”€â”€ __init__.py              # Test package initialization
â”œâ”€â”€ conftest.py              # Pytest configuration and shared fixtures
â”œâ”€â”€ test_models.py           # Tests for Pydantic models
â”œâ”€â”€ test_rag_system.py       # Tests for RAG system core functionality
â”œâ”€â”€ test_api_endpoints.py    # Tests for FastAPI endpoints
â”œâ”€â”€ test_utils.py            # Tests for utility functions
â””â”€â”€ README.md                # This file
```

## ðŸ§ª Test Categories

### 1. **Unit Tests** (`test_models.py`, `test_utils.py`)
- **Purpose**: Test individual components in isolation
- **Scope**: Pydantic models, utility functions, data validation
- **Speed**: Fast execution
- **Dependencies**: Minimal external dependencies

### 2. **Integration Tests** (`test_api_endpoints.py`)
- **Purpose**: Test API endpoints and their interactions
- **Scope**: FastAPI endpoints, request/response handling
- **Speed**: Medium execution
- **Dependencies**: FastAPI TestClient, mocked RAG system

### 3. **System Tests** (`test_rag_system.py`)
- **Purpose**: Test the complete RAG system functionality
- **Scope**: LangChain components, document processing, confidence calculation
- **Speed**: Slow execution (marked with `@pytest.mark.slow`)
- **Dependencies**: Mocked external APIs and services

## ðŸš€ Running Tests

### Using Make Commands
```bash
# Run all tests
make test

# Run unit tests only
make test-unit

# Run integration tests only
make test-integration

# Run tests with coverage
make test-coverage

# Run fast tests (excluding slow tests)
make test-fast
```

### Using Pytest Directly
```bash
# Run all tests
pytest tests/ -v

# Run specific test file
pytest tests/test_models.py -v

# Run tests with specific marker
pytest tests/ -m unit -v
pytest tests/ -m integration -v
pytest tests/ -m "not slow" -v

# Run tests with coverage
pytest tests/ --cov=src --cov-report=html --cov-report=term-missing
```

### Using Make Commands (Recommended)
```bash
# Run all tests
make test

# Run specific test types
make test-unit
make test-integration
make test-fast
make test-coverage
```

## ðŸ“Š Test Coverage

The test suite aims to provide comprehensive coverage of:

- **Pydantic Models**: 100% validation testing
- **API Endpoints**: All endpoints with success/error scenarios
- **RAG System**: Core functionality with mocked dependencies
- **Utility Functions**: Data validation, error handling, formatting
- **Error Handling**: Edge cases and error scenarios

## ðŸ”§ Test Configuration

### Pytest Configuration (`conftest.py`)
- **Fixtures**: Shared test data and mock objects
- **Markers**: Test categorization (unit, integration, slow)
- **Path Setup**: Automatic src/ directory import
- **Environment**: Test environment variables

### Test Markers
- `@pytest.mark.unit`: Fast unit tests
- `@pytest.mark.integration`: API integration tests
- `@pytest.mark.slow`: Tests that take longer to execute

## ðŸŽ¯ Test Scenarios

### Model Validation Tests
- âœ… Valid data creation
- âœ… Invalid data rejection
- âœ… Edge case handling
- âœ… Type validation
- âœ… Range validation

### API Endpoint Tests
- âœ… Successful requests
- âœ… Error handling (404, 500, 422)
- âœ… Invalid JSON handling
- âœ… Missing required fields
- âœ… Conversation history management

### RAG System Tests
- âœ… Component initialization
- âœ… Document loading
- âœ… Query processing
- âœ… Confidence calculation
- âœ… Error handling
- âœ… Cache management

### Utility Function Tests
- âœ… Data validation
- âœ… Error formatting
- âœ… Timestamp handling
- âœ… Category validation
- âœ… Conversation formatting

## ðŸ› ï¸ Mocking Strategy

### External Dependencies
- **OpenAI API**: Mocked responses for consistent testing
- **LangChain Components**: Mocked for isolated testing
- **File System**: Mocked document loading
- **Environment Variables**: Controlled test environment

### Test Data
- **Conversation History**: Predefined test conversations
- **Documents**: Mock document objects with metadata
- **API Responses**: Consistent mock responses
- **Error Scenarios**: Various error conditions

## ðŸ“ˆ Coverage Reports

After running tests with coverage, you can view detailed reports:

```bash
# Generate HTML coverage report
make test-coverage

# View coverage report
open htmlcov/index.html
```

## ðŸ” Debugging Tests

### Verbose Output
```bash
pytest tests/ -v -s
```

### Specific Test Debugging
```bash
# Run single test
pytest tests/test_models.py::TestChatRequest::test_valid_chat_request -v -s

# Run tests matching pattern
pytest tests/ -k "test_valid" -v
```

### Test Isolation
```bash
# Run tests in isolation
pytest tests/ --tb=short --maxfail=1
```

## ðŸš¨ Common Issues

### Import Errors
- Ensure you're running from the project root
- Check that `src/` is in the Python path
- Verify virtual environment is activated

### Mock Issues
- Check that mocks are properly configured
- Ensure async mocks are used for async functions
- Verify mock return values match expected types

### Environment Issues
- Set `TESTING=true` environment variable
- Ensure test environment variables are set
- Check that no real API calls are being made

## ðŸ“ Adding New Tests

### Test File Structure
```python
#!/usr/bin/env python3
"""
Unit tests for [component name]
"""

import pytest
from unittest.mock import Mock, patch

class TestComponentName:
    """Test [Component Name] class"""
    
    def test_specific_functionality(self):
        """Test specific functionality"""
        # Arrange
        # Act
        # Assert
```

### Test Naming Conventions
- Test classes: `Test[ComponentName]`
- Test methods: `test_[description]`
- Use descriptive names that explain the test scenario

### Test Organization
- Group related tests in the same class
- Use fixtures for shared setup
- Keep tests independent and isolated
- Use appropriate markers for categorization

## ðŸŽ‰ Success Criteria

A successful test run should show:
- âœ… All tests passing
- âœ… Good test coverage (>80%)
- âœ… No real API calls made
- âœ… Fast execution for unit tests
- âœ… Comprehensive error scenario coverage

